{"cells":[{"cell_type":"markdown","metadata":{"id":"FJ1A0ymWxax7"},"source":["__ResNet18__ \\\n","하나의 메뉴만 있는 사진으로 모델 학습 진행 \\\n","런타임 종료로 인해 epoch6에서 중단 \\\n","학습결과 Loss: 0.3156, Validation Accuracy: 74.52%"]},{"cell_type":"markdown","metadata":{"id":"I_yOhTi0A6Ts"},"source":["### 1. 데이터 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyS3TxGPINMx"},"outputs":[],"source":["# 구글 드라이브 마운트\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iX8DyzGRN9UN"},"outputs":[],"source":["import os\n","# 경로 설정\n","folder_path = '/content/drive/MyDrive/dataset/empty_labeling'\n","\n","# 파일 개수 세기\n","if os.path.exists(folder_path):\n","    num_files = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n","    print(f\"Total number of files: {num_files}\")\n","else:\n","    print(f\"The folder path {folder_path} does not exist.\")"]},{"cell_type":"markdown","metadata":{"id":"t3UOd1-TA9ht"},"source":["### 2. 이미지-라벨링 매핑"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tl4iUtMCcS40"},"outputs":[],"source":["import shutil\n","import os\n","\n","output_dir = '/content/drive/MyDrive'\n","resnet_dataset_path = '/content/drive/MyDrive/resnet_dataset'\n","\n","\n","# 압축 풀린 데이터셋 경로 설정\n","dataset_path = os.path.join(output_dir, 'dataset/kfood')  # kfood 폴더 경로\n","output_txts_path = os.path.join(output_dir, 'dataset/empty_labeling')  # empty_labeling 폴더 경로\n","\n","# 대분류 폴더 순회\n","for category in os.listdir(dataset_path):\n","    category_path = os.path.join(dataset_path, category)\n","\n","    if os.path.isdir(category_path):\n","        # 중분류 폴더 순회\n","        for subcategory in os.listdir(category_path):\n","            subcategory_path = os.path.join(category_path, subcategory)\n","\n","            if os.path.isdir(subcategory_path):\n","                # 이미지와 properties 파일을 포함한 파일 탐색\n","                for img_file in os.listdir(subcategory_path):\n","                    if img_file.endswith(('.jpg', '.png', '.jpeg')):\n","                        img_path = os.path.join(subcategory_path, img_file)\n","\n","                        # 좌표 파일을 empty_labeling에서 찾기\n","                        base_name = os.path.splitext(img_file)[0]\n","                        txt_file = f'{base_name}.txt'\n","                        txt_file_path = os.path.join(output_txts_path, txt_file)\n","\n","                        # 텍스트 파일이 있는 경우에만 복사\n","                        if os.path.exists(txt_file_path):\n","                            print(f\"Copying image: {img_path}\")\n","                            shutil.copy(img_path, resnet_dataset_path)\n","                            print(f\"Copying label: {txt_file_path}\")\n","                            shutil.copy(txt_file_path, resnet_dataset_path)\n","                        else:\n","                            print(f\"Label not found for: {img_file}, skipping image.\")"]},{"cell_type":"markdown","metadata":{"id":"88eeOzttBDYZ"},"source":["### 3. 모델링"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":688},"id":"JFbGmJK0X_KF","outputId":"7d6b08c4-7244-4ee1-e05c-9d4ec9c60834"},"outputs":[{"name":"stdout","output_type":"stream","text":["총 148244개의 유효한 이미지가 발견되었습니다.\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 193MB/s]\n","/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n","  warnings.warn(str(msg))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Loss: 1.9127, Validation Accuracy: 67.23%\n","Epoch [2/10], Loss: 1.1009, Validation Accuracy: 70.88%\n","Epoch [3/10], Loss: 0.8211, Validation Accuracy: 70.32%\n","Epoch [4/10], Loss: 0.6183, Validation Accuracy: 73.64%\n","Epoch [5/10], Loss: 0.4499, Validation Accuracy: 74.00%\n","Epoch [6/10], Loss: 0.3156, Validation Accuracy: 74.52%\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-ca8911d830d6>\u001b[0m in \u001b[0;36m<cell line: 154>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;31m# 학습 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;31m# 학습된 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-ca8911d830d6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# ResNet18 모델링 코드\n","\n","from PIL import Image\n","import torch\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","\n","# empty_labeling 폴더에 있는 txt 파일 이름을 불러오는 함수\n","def get_valid_image_names(empty_labeling_dir):\n","    valid_image_names = set()\n","    for file in os.listdir(empty_labeling_dir):\n","        if file.endswith('.txt'):\n","            # .txt를 제거하고 이미지 파일 이름으로 사용\n","            image_name = file.replace('.txt', '')\n","            valid_image_names.add(image_name)\n","    return valid_image_names\n","\n","# 이미지 경로와 라벨을 자동으로 생성하는 함수\n","def create_image_label_list(root_dir, valid_image_names):\n","    image_paths = []\n","    labels = []\n","    label_dict = {}\n","    label_id = 0\n","\n","    for root, dirs, files in os.walk(root_dir):\n","        # 각 폴더를 카테고리로 처리\n","        if files:\n","            category = os.path.basename(root)\n","            if category not in label_dict:\n","                label_dict[category] = label_id\n","                label_id += 1\n","            # 파일이 있는 경우에만 이미지 경로와 라벨 추가\n","            for file in files:\n","                # 이미지가 valid_image_names에 있을 때만 처리\n","                if file.lower().endswith(('.jpg', '.jpeg', '.png')) and os.path.splitext(file)[0] in valid_image_names:\n","                    image_paths.append(os.path.join(root, file))\n","                    labels.append(label_dict[category])\n","\n","    return image_paths, labels, label_dict\n","\n","# 데이터셋 클래스 정의\n","class KoreanFoodDataset(torch.utils.data.Dataset):\n","    def __init__(self, image_paths, labels, transform=None):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        label = self.labels[idx]\n","        return image, label\n","\n","# ResNet 모델 설정\n","def get_resnet_model(num_classes):\n","    model = resnet18(pretrained=True)\n","    in_features = model.fc.in_features\n","    model.fc = torch.nn.Linear(in_features, num_classes)\n","    return model\n","\n","# 학습 및 평가 함수\n","def train_model(model, train_loader, valid_loader, num_epochs, device):\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","    best_accuracy = 0\n","    best_model_wts = None\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        # Training loop\n","        for images, labels in train_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","        # Validation loop\n","        model.eval()\n","        total = 0\n","        correct = 0\n","        with torch.no_grad():\n","            for images, labels in valid_loader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        validation_acc = 100 * correct / total if total > 0 else 0\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Validation Accuracy: {validation_acc:.2f}%\")\n","\n","        # 성능이 더 좋으면 저장\n","        if validation_acc > best_accuracy:\n","            best_accuracy = validation_acc\n","            best_model_wts = model.state_dict()\n","\n","    # 성능이 가장 좋았던 모델 저장\n","    torch.save(best_model_wts, '/content/drive/MyDrive/korean_food_resnet_best.pth')\n","\n","# 경로 설정 및 데이터 준비\n","empty_labeling_dir = '/content/drive/MyDrive/dataset/empty_labeling'\n","root_dir = '/content/drive/MyDrive/dataset/kfood'\n","\n","# valid_image_names 생성\n","valid_image_names = get_valid_image_names(empty_labeling_dir)\n","\n","# 이미지 경로와 라벨 생성\n","image_paths, labels, label_dict = create_image_label_list(root_dir, valid_image_names)\n","print(f\"총 {len(image_paths)}개의 유효한 이미지가 발견되었습니다.\")\n","\n","# 이미지 크기 조정 및 전처리\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # ResNet의 기본 입력 크기는 224x224\n","    transforms.ToTensor()\n","])\n","\n","# 데이터셋 및 DataLoader 준비\n","dataset = KoreanFoodDataset(image_paths, labels, transform=transform)\n","\n","# 데이터셋 나누기 (데이터가 충분하지 않은 경우 나누기 어려움)\n","train_size = int(0.7 * len(dataset))\n","valid_size = len(dataset) - train_size\n","train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n","\n","# 모델 설정 및 학습\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","num_classes = len(label_dict)  # 카테고리 수\n","model = get_resnet_model(num_classes)\n","model.to(device)\n","\n","# 학습 시작\n","train_model(model, train_loader, valid_loader, num_epochs=10, device=device)\n","\n","# 학습된 모델 저장\n","torch.save(model.state_dict(), '/content/drive/MyDrive/korean_food_resnet_full.pth')"]},{"cell_type":"markdown","metadata":{"id":"JC3tCFebxax_"},"source":["__런타임이 끊겨 epoch 6에서 학습이 중단__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_6FtRAR097w"},"outputs":[],"source":["def load_model(model_path, num_classes, device):\n","    model = get_resnet_model(num_classes)\n","    model.load_state_dict(torch.load(model_path))\n","    model.to(device)\n","    model.eval()  # 평가 모드로 설정 (학습 중 적용되었던 dropout 등을 비활성화)\n","    return model\n","\n","def evaluate_model(model, test_loader, device):\n","    model.eval()  # 평가 모드로 설정\n","    total = 0\n","    correct = 0\n","    all_labels = []\n","    all_preds = []\n","\n","    with torch.no_grad():  # 평가 시에는 gradient 계산을 하지 않음\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            # 실제 라벨과 예측 라벨 저장 (추후 성능 지표 계산을 위해)\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(predicted.cpu().numpy())\n","\n","    accuracy = 100 * correct / total if total > 0 else 0\n","    print(f\"테스트 데이터에서 정확도: {accuracy:.2f}%\")\n","    return all_labels, all_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEVyvla01HMQ"},"outputs":[],"source":["# 예시: 검증 데이터셋을 테스트 데이터셋으로 재사용\n","test_loader = valid_loader  # 검증 데이터셋을 테스트 데이터셋으로 사용 가능\n","\n","# 별도로 테스트 데이터가 있다면, 해당 데이터셋을 DataLoader로 로드\n","# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","\n","# 모델 경로와 평가할 데이터 준비\n","model_path = '/content/drive/MyDrive/korean_food_resnet_best.pth'\n","num_classes = len(label_dict)\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# 학습된 모델 불러오기\n","model = load_model(model_path, num_classes, device)\n","\n","# 평가 수행\n","all_labels, all_preds = evaluate_model(model, test_loader, device)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}